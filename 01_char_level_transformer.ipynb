{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level Transformer (Java autocomplete)\n",
    "\n",
    "In this notebook, I try to make a small mini-GPT - a simple version of the kind of model used in ChatGPT or GitHub Copilot.\n",
    "\n",
    "When I say \"character-level\" I mean the model gets characters, not tokens. For example:\n",
    "- input: `public void get`\n",
    "- model should predict next char: `N` (so it can become `getName`)\n",
    "\n",
    "It is easier than subword tokenization, but idea is same.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 — Setup & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: Apple Silicon GPU (MPS) — I use it\n",
      "PyTorch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# I check which device is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    print(\"Device: Apple Silicon GPU (MPS) — I use it\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU (for Colab)\n",
    "    print(\"Device: CUDA GPU — I use it\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Device: CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Character-Level Transformer for Java Autocomplete\n",
    "\n",
    "## Lecture Notes: Building a Mini Language Model from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "## What Are We Building?\n",
    "\n",
    "The goal of this notebook is to build a **mini-GPT** — a small-scale version of the same architecture that powers ChatGPT, GitHub Copilot, and other modern language models.\n",
    "\n",
    "**Character-level** means the model will:\n",
    "- Take a sequence of characters as input: `public void get`\n",
    "- Predict the next character: `N` (→ completing to `getName`)\n",
    "\n",
    "This approach is simpler than working with tokens (words or subwords), but the underlying principles are exactly the same. Once this is understood, scaling up to production-level models becomes much more intuitive.\n",
    "\n",
    "### Why Start With Character-Level?\n",
    "\n",
    "Working at the character level has pedagogical advantages:\n",
    "- **Smaller vocabulary**: Instead of 50,000+ tokens, there are only ~100 unique characters\n",
    "- **Easier to debug**: The model's behavior is more transparent\n",
    "- **Same architecture**: The Transformer architecture is identical to production models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "PyTorch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check available compute devices\n",
    "# Modern deep learning benefits enormously from GPU acceleration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU (for Colab or workstations)\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "### Background: How Does a Neural Network \"See\" Text?\n",
    "\n",
    "Neural networks operate exclusively on numbers — they cannot directly process characters, words, or any symbolic data. This means every piece of text must be converted into numerical form before the model can work with it.\n",
    "\n",
    "The conversion process involves three steps:\n",
    "\n",
    "1. **Build a vocabulary** — create a mapping from each unique character to a number (an index)\n",
    "2. **Encode** — convert text into a sequence of these indices\n",
    "3. **Decode** — convert indices back to text for human-readable output\n",
    "\n",
    "**Concrete Example:**\n",
    "```\n",
    "Vocabulary: {'p': 0, 'u': 1, 'b': 2, 'l': 3, 'i': 4, 'c': 5, ' ': 6, ...}\n",
    "Text:       \"public\"\n",
    "Encoded:    [0, 1, 2, 3, 4, 5]\n",
    "```\n",
    "\n",
    "### Quick Math Refresher: One-Hot Encoding\n",
    "\n",
    "Later, these indices get converted into **one-hot vectors** or looked up in an **embedding table**. A one-hot vector has all zeros except for a single 1 at the index position:\n",
    "\n",
    "- If vocabulary size = 10 and character index = 3:\n",
    "- One-hot: `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`\n",
    "\n",
    "In practice, models use **embeddings** instead — dense vectors that are learned during training. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1025 characters\n"
     ]
    }
   ],
   "source": [
    "# Starting with a small Java code sample for quick experimentation\n",
    "# Later, this can be replaced with a real dataset of thousands of files\n",
    "\n",
    "SAMPLE_JAVA_CODE = '''\n",
    "public class HelloWorld {\n",
    "    public static void main(String[] args) {\n",
    "        System.out.println(\"Hello, World!\");\n",
    "    }\n",
    "}\n",
    "\n",
    "public class Calculator {\n",
    "    private int value;\n",
    "    \n",
    "    public Calculator() {\n",
    "        this.value = 0;\n",
    "    }\n",
    "    \n",
    "    public int add(int x) {\n",
    "        this.value += x;\n",
    "        return this.value;\n",
    "    }\n",
    "    \n",
    "    public int subtract(int x) {\n",
    "        this.value -= x;\n",
    "        return this.value;\n",
    "    }\n",
    "    \n",
    "    public int getValue() {\n",
    "        return this.value;\n",
    "    }\n",
    "    \n",
    "    public void setValue(int value) {\n",
    "        this.value = value;\n",
    "    }\n",
    "}\n",
    "\n",
    "public class StringUtils {\n",
    "    public static String reverse(String s) {\n",
    "        StringBuilder sb = new StringBuilder(s);\n",
    "        return sb.reverse().toString();\n",
    "    }\n",
    "    \n",
    "    public static boolean isEmpty(String s) {\n",
    "        return s == null || s.length() == 0;\n",
    "    }\n",
    "    \n",
    "    public static String capitalize(String s) {\n",
    "        if (isEmpty(s)) {\n",
    "            return s;\n",
    "        }\n",
    "        return Character.toUpperCase(s.charAt(0)) + s.substring(1);\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(f\"Sample size: {len(SAMPLE_JAVA_CODE)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created: 51 unique characters\n",
      "Examples: [('\\n', 0), (' ', 1), ('!', 2), ('\"', 3), ('(', 4), (')', 5), ('+', 6), (',', 7), ('-', 8), ('.', 9)]...\n",
      "\n",
      "Roundtrip test: 'public void' → [38, 42, 26, 34, 33, 27, 1, 43, 37, 33, 28] → 'public void'\n"
     ]
    }
   ],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    A simple character-level tokenizer.\n",
    "    \n",
    "    This class builds a vocabulary from all unique characters in the text\n",
    "    and provides methods to convert between text and numerical indices.\n",
    "    \n",
    "    Why \"tokenizer\"? In NLP, the process of breaking text into units is called\n",
    "    tokenization. Here, each token is a single character.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text: str):\n",
    "        # Collect all unique characters and sort them for reproducibility\n",
    "        # Sorting ensures the same text always produces the same vocabulary\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        # Create bidirectional mappings\n",
    "        # char_to_idx: 'a' → 0, 'b' → 1, etc.\n",
    "        # idx_to_char: 0 → 'a', 1 → 'b', etc.\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        \n",
    "        print(f\"Vocabulary created: {self.vocab_size} unique characters\")\n",
    "        print(f\"Examples: {list(self.char_to_idx.items())[:10]}...\")\n",
    "    \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Convert text to a list of integers (indices)\"\"\"\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        \"\"\"Convert a list of integers back to text\"\"\"\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])\n",
    "\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = CharTokenizer(SAMPLE_JAVA_CODE)\n",
    "\n",
    "# Verify that encode/decode are inverse operations (roundtrip test)\n",
    "test_text = \"public void\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nRoundtrip test: '{test_text}' → {encoded} → '{decoded}'\")\n",
    "assert test_text == decoded, \"Tokenizer error: roundtrip failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: How Are Training Examples Formed?\n",
    "\n",
    "A language model learns to predict the **next token** given all previous tokens. This is sometimes called **autoregressive** modeling — the model's output at step N becomes part of the input for step N+1.\n",
    "\n",
    "From the text `\"public class\"`, the following training pairs (input → target) are created:\n",
    "\n",
    "```\n",
    "\"p\"           → \"u\"        (given 'p', predict 'u')\n",
    "\"pu\"          → \"b\"        (given 'pu', predict 'b')\n",
    "\"pub\"         → \"l\"        (given 'pub', predict 'l')\n",
    "\"publ\"        → \"i\"        \n",
    "\"publi\"       → \"c\"        \n",
    "\"public\"      → \" \"        (the space character!)\n",
    "\"public \"     → \"c\"        \n",
    "\"public c\"    → \"l\"        \n",
    "...\n",
    "```\n",
    "\n",
    "### Practical Implementation: Fixed Context Windows\n",
    "\n",
    "In practice, using variable-length inputs would be inefficient. Instead, a fixed **context length** (also called sequence length or block size) is used. All inputs are padded or truncated to this length.\n",
    "\n",
    "The clever trick: from one context window, multiple predictions can be made simultaneously. If the context length is 8, then from positions 0-7, the model predicts positions 1-8. This is much more efficient than making one prediction at a time.\n",
    "\n",
    "### Math Refresher: Tensors and Shapes\n",
    "\n",
    "Throughout this notebook, tensor shapes are annotated as `(B, T, C)` where:\n",
    "- **B** = Batch size (number of examples processed in parallel)\n",
    "- **T** = Time steps / Sequence length (number of tokens)\n",
    "- **C** = Channels / Features (dimension of each token's representation)\n",
    "\n",
    "This notation comes from signal processing and is standard in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 1025 tokens, 961 training examples\n",
      "\n",
      "Example training pair:\n",
      "x (input):  '\n",
      "public class HelloWorld {\n",
      "    public static void main(String[] '\n",
      "y (target): 'public class HelloWorld {\n",
      "    public static void main(String[] a'\n",
      "\n",
      "Tensor shapes: x=torch.Size([64]), y=torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for training a language model.\n",
    "    \n",
    "    Each example consists of:\n",
    "    - x: a sequence of context_length characters (the input)\n",
    "    - y: the same sequence shifted by 1 position (the target)\n",
    "    \n",
    "    Example with context_length=8:\n",
    "    - x: \"public v\"  [characters at positions 0-7]\n",
    "    - y: \"ublic vo\"  [characters at positions 1-8]\n",
    "    \n",
    "    The model learns:\n",
    "    - From x[0] alone, predict y[0]\n",
    "    - From x[0:2], predict y[1]\n",
    "    - From x[0:3], predict y[2]\n",
    "    - And so on...\n",
    "    \n",
    "    This is efficient because one forward pass produces context_length predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text: str, tokenizer: CharTokenizer, context_length: int):\n",
    "        self.context_length = context_length\n",
    "        # Convert entire text to a tensor of indices\n",
    "        # dtype=torch.long because indices must be integers\n",
    "        self.data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        print(f\"Dataset created: {len(self.data)} tokens, {len(self)} training examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of possible windows that can be extracted\n",
    "        # Subtract context_length because the last window needs one extra token for the target\n",
    "        return len(self.data) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract a window of [idx : idx + context_length + 1]\n",
    "        # Then split into input (x) and target (y)\n",
    "        chunk = self.data[idx : idx + self.context_length + 1]\n",
    "        x = chunk[:-1]  # Everything except the last token\n",
    "        y = chunk[1:]   # Everything except the first token\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Create the dataset with a modest context length to start\n",
    "CONTEXT_LENGTH = 64  # The model will \"see\" up to 64 characters at a time\n",
    "dataset = CharDataset(SAMPLE_JAVA_CODE, tokenizer, CONTEXT_LENGTH)\n",
    "\n",
    "# Examine one training example\n",
    "x, y = dataset[0]\n",
    "print(f\"\\nExample training pair:\")\n",
    "print(f\"x (input):  '{tokenizer.decode(x.tolist())}'\")\n",
    "print(f\"y (target): '{tokenizer.decode(y.tolist())}'\")\n",
    "print(f\"\\nTensor shapes: x={x.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: The Transformer Architecture\n",
    "\n",
    "### Background: Why Transformers?\n",
    "\n",
    "Before 2017, sequence modeling (text, speech, time series) was dominated by **Recurrent Neural Networks (RNNs)** and their variants like **LSTMs** (Long Short-Term Memory). These had significant limitations:\n",
    "\n",
    "1. **Sequential processing**: RNNs must process tokens one at a time, making parallelization impossible. This means slow training and inference.\n",
    "\n",
    "2. **Vanishing gradients**: Information from early tokens gets \"diluted\" as it passes through many steps. Learning long-range dependencies (like matching an opening brace with its closing brace 100 characters later) becomes very difficult.\n",
    "\n",
    "The **Transformer** architecture, introduced in the paper \"Attention Is All You Need\" (2017), solves both problems through a mechanism called **Self-Attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: What is Self-Attention?\n",
    "\n",
    "**The intuition:** Each token \"looks at\" all other tokens (or in our case, all *previous* tokens) and decides how much attention to pay to each one.\n",
    "\n",
    "**Concrete example:** In the code `this.value = x;`, when the model reaches `x`, it should \"attend to\":\n",
    "- `value` — to understand what's being assigned\n",
    "- `this` — to understand the context (it's inside a class method)\n",
    "- `.` and `=` — to understand the syntactic structure\n",
    "\n",
    "### The Query-Key-Value Mechanism\n",
    "\n",
    "Each token creates three different vectors:\n",
    "\n",
    "1. **Query (Q)**: \"What am I looking for?\"\n",
    "   - Think of this as a question the token is asking\n",
    "   \n",
    "2. **Key (K)**: \"What do I contain?\"\n",
    "   - Think of this as a label or tag describing the token's content\n",
    "   \n",
    "3. **Value (V)**: \"What information do I provide if someone attends to me?\"\n",
    "   - This is the actual information that gets passed along\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "1. **$QK^T$** — Compute \"similarity\" between each query and all keys\n",
    "   - Matrix multiplication: if Q is (T, d) and K is (T, d), then QK^T is (T, T)\n",
    "   - Entry (i, j) tells us how much token i's query matches token j's key\n",
    "\n",
    "2. **$\\div \\sqrt{d_k}$** — Scale by the square root of the key dimension\n",
    "   - **Why?** Without scaling, the dot products can become very large when d_k is big\n",
    "   - Large values push softmax into regions with tiny gradients (saturation)\n",
    "   - Dividing by $\\sqrt{d_k}$ keeps values in a reasonable range\n",
    "\n",
    "3. **softmax** — Convert scores to probabilities (sum to 1)\n",
    "   - Each row becomes a probability distribution over all tokens\n",
    "   - Higher scores → higher probabilities\n",
    "\n",
    "4. **$\\times V$** — Weighted sum of values\n",
    "   - Each token's output is a mixture of all values, weighted by attention\n",
    "\n",
    "### Math Refresher: Softmax\n",
    "\n",
    "Softmax converts a vector of arbitrary real numbers into a probability distribution:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "Properties:\n",
    "- All outputs are positive (due to exponential)\n",
    "- All outputs sum to 1\n",
    "- Larger inputs get larger probabilities\n",
    "- The exponential makes it \"winner-take-more\" — differences are amplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask (8x8 example):\n",
      "1 = can see, 0 = cannot see (future)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A single \"head\" of self-attention.\n",
    "    \n",
    "    Parameters:\n",
    "    - embed_dim: the dimension of token embeddings (input size)\n",
    "    - head_dim: the dimension of Q, K, V vectors in this head\n",
    "    - context_length: maximum sequence length (needed for the causal mask)\n",
    "    \n",
    "    Note: In multi-head attention, head_dim is typically embed_dim // num_heads,\n",
    "    so all heads together have the same total dimensionality as embed_dim.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, head_dim: int, context_length: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projections to create Q, K, V from input embeddings\n",
    "        # These are learned transformations: input → query/key/value space\n",
    "        # bias=False is common in attention (the original paper used no bias)\n",
    "        self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        \n",
    "        # Causal mask: prevents attending to future tokens\n",
    "        # This is essential for autoregressive generation\n",
    "        # A lower triangular matrix: position i can only see positions 0..i\n",
    "        #\n",
    "        # Example for 4 positions:\n",
    "        # [[1, 0, 0, 0],   Position 0 sees only itself\n",
    "        #  [1, 1, 0, 0],   Position 1 sees positions 0, 1\n",
    "        #  [1, 1, 1, 0],   Position 2 sees positions 0, 1, 2\n",
    "        #  [1, 1, 1, 1]]   Position 3 sees all positions\n",
    "        mask = torch.tril(torch.ones(context_length, context_length))\n",
    "        self.register_buffer('mask', mask)  # register_buffer: saved with model but not trained\n",
    "        \n",
    "        # Scale factor: 1/sqrt(d_k) as in the attention formula\n",
    "        self.scale = head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch, Time (sequence length), Channels (embed_dim)\n",
    "        \n",
    "        # Project input into query, key, value spaces\n",
    "        q = self.query(x)  # (B, T, head_dim)\n",
    "        k = self.key(x)    # (B, T, head_dim)\n",
    "        v = self.value(x)  # (B, T, head_dim)\n",
    "        \n",
    "        # Compute attention scores: how much each token attends to each other token\n",
    "        # q @ k.transpose(-2, -1) produces a (T, T) matrix per batch element\n",
    "        # Entry (i, j) = dot product of query_i and key_j = \"relevance\" of j to i\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask: set \"future\" positions to -infinity\n",
    "        # After softmax, -inf becomes 0 (e^(-inf) = 0)\n",
    "        # This ensures each position only attends to itself and past positions\n",
    "        scores = scores.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax along last dimension: each row becomes a probability distribution\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # Weighted sum of values: each output is a mixture of all values\n",
    "        # weighted by the attention probabilities\n",
    "        out = attn_weights @ v  # (B, T, head_dim)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Visualize the causal mask structure\n",
    "print(\"Causal mask (8x8 example):\")\n",
    "print(\"1 = can see, 0 = cannot see (future)\")\n",
    "print(torch.tril(torch.ones(8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention: Why Multiple Heads?\n",
    "\n",
    "A single attention head can only focus on one type of relationship at a time. **Multi-head attention** runs several attention mechanisms in parallel, each with its own Q, K, V projections.\n",
    "\n",
    "**Intuition:** Different heads learn to focus on different aspects:\n",
    "- Head 1: Might track syntactic structure (braces, semicolons, parentheses)\n",
    "- Head 2: Might track data types (int, String, boolean)\n",
    "- Head 3: Might track variable names and their usage\n",
    "- Head 4: Might track control flow (if, else, return)\n",
    "\n",
    "The outputs of all heads are concatenated and then projected back to the original dimension.\n",
    "\n",
    "### Math Details\n",
    "\n",
    "If there are $h$ heads and the total embedding dimension is $d_{model}$:\n",
    "- Each head has dimension $d_k = d_{model} / h$\n",
    "- After concatenation: $h \\times d_k = d_{model}$ (back to original size)\n",
    "- A final linear layer projects this concatenation\n",
    "\n",
    "This design keeps the computational cost similar to single-head attention of the same dimension, while allowing the model to attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention.\n",
    "    \n",
    "    Runs multiple SelfAttention heads in parallel,\n",
    "    concatenates their outputs, and projects back to embed_dim.\n",
    "    \n",
    "    This allows the model to jointly attend to information\n",
    "    from different representation subspaces at different positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, context_length: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embed_dim must be divisible by num_heads so each head gets equal dimensions\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Create multiple attention heads\n",
    "        # nn.ModuleList ensures PyTorch tracks these as submodules\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(embed_dim, head_dim, context_length)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Final projection after concatenating all heads\n",
    "        # This allows the model to mix information across heads\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Run all heads and concatenate their outputs\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Project concatenated output\n",
    "        out = self.proj(concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Network (FFN): Adding Non-linearity\n",
    "\n",
    "After attention, each position passes through a simple two-layer neural network:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**Why is this needed?**\n",
    "\n",
    "Attention only \"mixes\" information between positions — it computes weighted averages. This is fundamentally a linear operation (weighted sums).\n",
    "\n",
    "The FFN adds **non-linear transformation** at each position independently. This is where much of the model's \"reasoning\" capacity comes from.\n",
    "\n",
    "**Architecture details:**\n",
    "- First layer expands dimension (typically 4× the embedding dimension)\n",
    "- GELU activation provides non-linearity\n",
    "- Second layer projects back to original dimension\n",
    "\n",
    "### Math Refresher: GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "GELU is a smooth activation function, similar to ReLU but differentiable everywhere:\n",
    "\n",
    "$$\\text{GELU}(x) = x \\cdot \\Phi(x)$$\n",
    "\n",
    "where $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "**Intuition:** GELU smoothly gates values based on how \"likely\" they are to be positive. Unlike ReLU (which has a hard cutoff at 0), GELU has a smooth transition, which often leads to better gradient flow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    Structure: Linear → GELU → Linear\n",
    "    \n",
    "    The inner dimension (ff_dim) is typically 4x the embedding dimension.\n",
    "    This \"expand then contract\" pattern is common in transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, ff_dim: int = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim  # Standard ratio from the original Transformer paper\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),  # Smoother than ReLU, works better for transformers\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Block: Putting It Together\n",
    "\n",
    "One transformer block combines attention and FFN with two critical additions:\n",
    "\n",
    "```\n",
    "x → LayerNorm → MultiHeadAttention → + (residual) → LayerNorm → FFN → + (residual) → out\n",
    "    └──────────────────────────────────────────────┘            └───────────────────────┘\n",
    "```\n",
    "\n",
    "### Residual Connections (Skip Connections)\n",
    "\n",
    "The `+` signs indicate **residual connections**: adding the input directly to the output.\n",
    "\n",
    "```python\n",
    "out = x + layer(x)  # Instead of just: out = layer(x)\n",
    "```\n",
    "\n",
    "**Why residual connections?**\n",
    "\n",
    "1. **Gradient flow**: In deep networks, gradients can vanish (approach zero) as they propagate backward through many layers. Residual connections provide a \"highway\" for gradients to flow directly.\n",
    "\n",
    "2. **Easier optimization**: The layer only needs to learn the \"residual\" — what to add to the input — rather than the full transformation. This is often easier.\n",
    "\n",
    "3. **Identity initialization**: At the start of training, if layer outputs are near zero, the block acts like an identity function. This makes training more stable.\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Layer normalization normalizes activations across the feature dimension:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation computed over the features, and $\\gamma$, $\\beta$ are learned parameters.\n",
    "\n",
    "**Why normalize?**\n",
    "- Keeps activations in a reasonable range\n",
    "- Reduces internal covariate shift (changes in input distribution during training)\n",
    "- Makes training more stable and often faster\n",
    "\n",
    "**Pre-LN vs Post-LN:** GPT uses \"Pre-LN\" — normalization *before* each sublayer. This tends to be more stable for training deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block: Attention + FFN with residual connections.\n",
    "    \n",
    "    Uses Pre-LN architecture (normalization before each sublayer),\n",
    "    which is the standard for GPT-style models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, context_length: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(embed_dim)  # Normalization before attention\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, context_length)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)  # Normalization before FFN\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-LN architecture (as used in GPT-2/3)\n",
    "        # Note: residual connection adds input to output of each sublayer\n",
    "        x = x + self.attn(self.ln1(x))  # Attention with residual\n",
    "        x = x + self.ffn(self.ln2(x))   # FFN with residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding: Teaching Order to Attention\n",
    "\n",
    "**The problem:** Self-attention by itself has no notion of position or order!\n",
    "\n",
    "For attention, `\"public class\"` and `\"class public\"` produce the same output (just reordered). This is because attention only computes pairwise similarities — it doesn't know which token came first.\n",
    "\n",
    "**The solution:** Add positional information to the token embeddings.\n",
    "\n",
    "In GPT-style models, this is done with **learned positional embeddings** — a lookup table where each position (0, 1, 2, ...) has its own learned vector:\n",
    "\n",
    "```\n",
    "Position 0 → vector [0.1, -0.3, 0.2, ...]\n",
    "Position 1 → vector [0.2, 0.5, -0.1, ...]\n",
    "Position 2 → vector [-0.1, 0.4, 0.3, ...]\n",
    "...\n",
    "```\n",
    "\n",
    "These vectors are added to the token embeddings:\n",
    "\n",
    "$$\\text{input} = \\text{TokenEmbedding}(\\text{token}) + \\text{PositionEmbedding}(\\text{position})$$\n",
    "\n",
    "The model learns what position means during training — for example, that position 0 is often the start of a statement, or that certain syntax patterns appear at specific relative positions.\n",
    "\n",
    "**Alternative: Sinusoidal encodings** (from the original Transformer paper) use fixed sine and cosine functions. Learned embeddings are more flexible and are the standard choice for GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Assembling the Complete Model\n",
    "\n",
    "Now all the pieces come together into the complete mini-GPT architecture:\n",
    "\n",
    "1. **Token Embedding**: Convert character indices to dense vectors\n",
    "2. **Position Embedding**: Add positional information  \n",
    "3. **N × Transformer Blocks**: The core processing\n",
    "4. **Final LayerNorm**: Stabilize outputs\n",
    "5. **Output Projection**: Convert vectors to logits over vocabulary\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Proper initialization is crucial for training stability. The standard approach for transformers:\n",
    "- Linear layers: Normal distribution with std=0.02\n",
    "- Embeddings: Normal distribution with std=0.02\n",
    "- Biases: Initialize to zero\n",
    "\n",
    "This keeps initial activations small but not zero, allowing gradients to flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: 813,107 parameters (0.81M)\n",
      "\n",
      "Forward pass test: input torch.Size([2, 32]) → output torch.Size([2, 32, 51])\n"
     ]
    }
   ],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A miniature GPT model for text generation.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Token Embedding: character → vector\n",
    "    2. Position Embedding: position → vector  \n",
    "    3. N × Transformer Blocks\n",
    "    4. Final LayerNorm\n",
    "    5. Output projection: vector → logits for each character in vocabulary\n",
    "    \n",
    "    The output logits can be converted to probabilities with softmax.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int = 128,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 4,\n",
    "        context_length: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        \n",
    "        # Embedding layers\n",
    "        # Token embedding: maps each vocabulary index to a dense vector\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Position embedding: maps each position (0 to context_length-1) to a dense vector\n",
    "        self.position_embedding = nn.Embedding(context_length, embed_dim)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        # nn.Sequential applies them in order\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, context_length)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization and projection to vocabulary\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights (important for stable training)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Count and display parameters\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Model created: {n_params:,} parameters ({n_params/1e6:.2f}M)\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Standard initialization for transformer models\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: (B, T) tensor of token indices\n",
    "            \n",
    "        Returns:\n",
    "            (B, T, vocab_size) tensor of logits for each position\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Get token embeddings: look up each index in the embedding table\n",
    "        tok_emb = self.token_embedding(idx)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Get position embeddings: create position indices [0, 1, 2, ..., T-1]\n",
    "        pos = torch.arange(T, device=idx.device)  # (T,)\n",
    "        pos_emb = self.position_embedding(pos)  # (T, embed_dim)\n",
    "        \n",
    "        # Add embeddings together\n",
    "        # Broadcasting: pos_emb (T, embed_dim) is broadcast across batch dimension\n",
    "        x = tok_emb + pos_emb  # (B, T, embed_dim)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # Project to vocabulary size to get logits\n",
    "        # Logits are unnormalized scores; apply softmax to get probabilities\n",
    "        logits = self.output_proj(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()  # Disable gradient computation for inference\n",
    "    def generate(self, idx, max_new_tokens: int, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: (B, T) tensor of starting context token indices\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: controls randomness\n",
    "                - temperature > 1.0: more random (flatter distribution)\n",
    "                - temperature < 1.0: more deterministic (sharper distribution)\n",
    "                - temperature = 1.0: use raw model probabilities\n",
    "        \n",
    "        Returns:\n",
    "            (B, T + max_new_tokens) tensor with generated tokens appended\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if it's longer than context_length\n",
    "            # The model can only see context_length tokens at a time\n",
    "            idx_cond = idx[:, -self.context_length:]\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = self(idx_cond)  # (B, T, vocab_size)\n",
    "            \n",
    "            # Focus on the last position only (next token prediction)\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            # multinomial: randomly selects indices according to probabilities\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append to the sequence\n",
    "            idx = torch.cat([idx, next_token], dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = MiniGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=128,      # Size of token vectors\n",
    "    num_heads=4,        # Number of attention heads\n",
    "    num_layers=4,       # Number of transformer blocks\n",
    "    context_length=CONTEXT_LENGTH,\n",
    ").to(device)\n",
    "\n",
    "# Verify that forward pass works with a test input\n",
    "test_input = torch.randint(0, tokenizer.vocab_size, (2, 32)).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nForward pass test: input {test_input.shape} → output {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Training\n",
    "\n",
    "### Background: The Loss Function\n",
    "\n",
    "Training uses **Cross-Entropy Loss** — the standard choice for classification problems.\n",
    "\n",
    "For each position in the sequence, the model outputs probabilities for all possible next characters. The loss measures how well the model's predicted probabilities match reality (where the \"correct\" character should have probability 1 and all others should have probability 0).\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i} \\log P(y_i | x_{<i})$$\n",
    "\n",
    "**Breaking this down:**\n",
    "\n",
    "- $P(y_i | x_{<i})$: The model's predicted probability for the correct next character $y_i$, given all previous characters $x_{<i}$\n",
    "- $\\log P$: Taking the logarithm (base e, i.e., natural log)\n",
    "- $-\\log P$: Negative log (since $0 < P ≤ 1$, $\\log P$ is negative or zero; negating makes it positive)\n",
    "- $\\sum_i$: Sum over all positions\n",
    "- $\\frac{1}{N}$: Average over all positions\n",
    "\n",
    "**Intuition:**\n",
    "- If the model assigns high probability to the correct character → $-\\log P$ is small → low loss\n",
    "- If the model assigns low probability to the correct character → $-\\log P$ is large → high loss\n",
    "\n",
    "### Math Refresher: Why Log-Probability?\n",
    "\n",
    "Using logarithms has several benefits:\n",
    "1. **Numerical stability**: Probabilities can be very small (like 0.0001); logs prevent underflow\n",
    "2. **Additive property**: $\\log(P_1 \\times P_2) = \\log P_1 + \\log P_2$, making joint probabilities easier to work with\n",
    "3. **Information theory**: $-\\log P$ is the number of bits needed to encode an event with probability $P$\n",
    "\n",
    "### Training Mechanics\n",
    "\n",
    "The training loop follows the standard pattern:\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Compute loss**: Compare predictions to targets\n",
    "3. **Backward pass**: Compute gradients (automatic differentiation)\n",
    "4. **Update weights**: Use optimizer to adjust parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 3e-4,\n",
    "    print_every: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for the language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The MiniGPT model to train\n",
    "        dataset: CharDataset containing training examples\n",
    "        tokenizer: CharTokenizer for encoding/decoding text\n",
    "        epochs: Number of complete passes through the dataset\n",
    "        batch_size: Number of examples per gradient update\n",
    "        learning_rate: Step size for optimization (3e-4 is a common default for transformers)\n",
    "        print_every: Print progress every N epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataLoader handles batching and shuffling\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # AdamW is the standard optimizer for transformers\n",
    "    # It's Adam with decoupled weight decay, which works better for this architecture\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set model to training mode (enables dropout, etc. if present)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            # Move data to the same device as the model\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            logits = model(x)  # (B, T, vocab_size)\n",
    "            \n",
    "            # Reshape for cross_entropy:\n",
    "            # - Predictions: (B*T, vocab_size) — one prediction per position\n",
    "            # - Targets: (B*T,) — one target index per position\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B * T, V),\n",
    "                y.view(B * T)\n",
    "            )\n",
    "            \n",
    "            # Backward pass: compute gradients\n",
    "            optimizer.zero_grad()  # Clear gradients from previous step\n",
    "            loss.backward()        # Compute gradients via backpropagation\n",
    "            optimizer.step()       # Update weights using gradients\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        # Print progress and generate a sample\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Generate a sample to see qualitative progress\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "            prompt = \"public \"\n",
    "            prompt_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "            generated = model.generate(prompt_ids, max_new_tokens=50, temperature=0.8)\n",
    "            generated_text = tokenizer.decode(generated[0].tolist())\n",
    "            print(f\"  Sample: {generated_text[:100]}...\")\n",
    "            model.train()  # Switch back to training mode\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 20/100, Loss: 0.1015\n",
      "  Sample: public void setValue(int value) {\n",
      "        this.value = va...\n",
      "Epoch 40/100, Loss: 0.0873\n",
      "  Sample: public static String capitalize(String s) {\n",
      "        if (i...\n",
      "Epoch 60/100, Loss: 0.0761\n",
      "  Sample: public Calculator() {\n",
      "        this.value = 0;\n",
      "    }\n",
      "    \n",
      "...\n",
      "Epoch 80/100, Loss: 0.0759\n",
      "  Sample: public class Calculator {\n",
      "    private int value;\n",
      "    \n",
      "   ...\n",
      "Epoch 100/100, Loss: 0.0706\n",
      "  Sample: public static String reverse(String s) {\n",
      "        StringBu...\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "model = train_model(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    epochs=100,           # Number of passes through the data\n",
    "    batch_size=16,        # Examples per gradient update\n",
    "    learning_rate=3e-4,   # Step size (3e-4 = 0.0003)\n",
    "    print_every=20,       # Print every 20 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Testing Generation\n",
    "\n",
    "Now the trained model can generate Java code completions. The generation process:\n",
    "\n",
    "1. Start with a prompt (partial code)\n",
    "2. Feed it through the model\n",
    "3. Sample the next character from the predicted distribution\n",
    "4. Append to the sequence and repeat\n",
    "\n",
    "### The Temperature Parameter\n",
    "\n",
    "Temperature controls the randomness of generation:\n",
    "\n",
    "- **temperature = 1.0**: Use the model's raw probabilities\n",
    "- **temperature < 1.0**: \"Sharper\" distribution — more likely to pick the highest-probability token (more deterministic)\n",
    "- **temperature > 1.0**: \"Flatter\" distribution — more uniform, more randomness (more creative/chaotic)\n",
    "\n",
    "Mathematically, temperature divides the logits before softmax:\n",
    "$$P_i = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}$$\n",
    "\n",
    "As $T \\to 0$, only the maximum logit gets probability 1 (argmax). As $T \\to \\infty$, the distribution becomes uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATION TESTING\n",
      "============================================================\n",
      "\n",
      "Prompt: 'public class '\n",
      "Result: public class Calculator {\n",
      "    private int value;\n",
      "    \n",
      "    public Calculat\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'public static void '\n",
      "Result: public static void main(String[] args) {\n",
      "        System.out.println(\"Hello, Wor\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'private int '\n",
      "Result: private int value;\n",
      "    \n",
      "    public Calculator() {\n",
      "        this.value = 0\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'return this.'\n",
      "Result: return this.value;\n",
      "    }\n",
      "    \n",
      "    public int getValue() {\n",
      "        return\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: 'if ('\n",
      "Result: if (isEmpty(s)) {\n",
      "            return s;\n",
      "        }\n",
      "        return\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_completion(model, tokenizer, prompt: str, max_tokens: int = 100, temperature: float = 0.8):\n",
    "    \"\"\"Generate a completion for the given prompt.\"\"\"\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    \n",
    "    prompt_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    generated = model.generate(prompt_ids, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "\n",
    "# Test with various prompts typical for Java code\n",
    "test_prompts = [\n",
    "    \"public class \",\n",
    "    \"public static void \",\n",
    "    \"private int \",\n",
    "    \"return this.\",\n",
    "    \"if (\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATION TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    completion = generate_completion(model, tokenizer, prompt, max_tokens=60)\n",
    "    print(f\"Result: {completion}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "\n",
    "### What Was Built\n",
    "\n",
    "This notebook implemented a complete mini-GPT from scratch:\n",
    "\n",
    "1. **Tokenization**: Converting characters to/from numerical indices\n",
    "2. **Self-Attention**: The core mechanism that allows tokens to communicate\n",
    "3. **Multi-Head Attention**: Parallel attention for different relationship types\n",
    "4. **Feed-Forward Networks**: Non-linear transformations at each position\n",
    "5. **Transformer Blocks**: Combining attention and FFN with residual connections\n",
    "6. **Positional Embeddings**: Teaching the model about token order\n",
    "7. **Training Loop**: Optimizing with cross-entropy loss\n",
    "8. **Text Generation**: Autoregressive sampling with temperature control\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "The model was trained on a tiny dataset (~1KB of code). It should already:\n",
    "- Learn basic Java syntax (braces, semicolons, indentation patterns)\n",
    "- Memorize patterns from the training examples\n",
    "\n",
    "But it won't generalize well to new patterns it hasn't seen.\n",
    "\n",
    "### What Comes Next\n",
    "\n",
    "To build a more capable model:\n",
    "\n",
    "1. **More data**: Load a real dataset with thousands of Java files\n",
    "2. **BPE tokenization**: Subword tokenization is more efficient for code than character-level\n",
    "3. **Scale up**: Increase embed_dim, num_layers, num_heads, and context_length\n",
    "4. **Proper evaluation**: Add train/validation split and track perplexity metrics\n",
    "5. **Regularization**: Add dropout to prevent overfitting on larger models\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "Before moving on, some experiments with this model:\n",
    "- Change hyperparameters (embed_dim, num_layers, num_heads) and observe effects\n",
    "- Add more Java code to SAMPLE_JAVA_CODE\n",
    "- Try different temperature values during generation (0.5 vs 1.0 vs 1.5)\n",
    "- Visualize attention patterns (which tokens attend to which?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
